# 展示车牌识别效果
大家好，下面我们将会开展使用yolo进行车牌目标识别的一系列视频    
希望可以吸引到对YOLO模型和目标识别技术感兴趣的同学或开发者    
后面我们会提供   
环境准备、数据集准备、训练模型、部署模型等一系列课程   
这里第一期视频，首先展示一下车牌的识别效果     

这里我们截取了20张图片   
我们会把这些图片的识别效果一一展示给大家   

在后面，我们会给大家提供视觉识别车牌的方法
尽请期待

# yolo识别原理与环境准备
大家好，这期视频给大家介绍yolo目标识别原理与环境准备，方便大家开展后续工作   

YOLO（You Only Look Once）是一种基于深度学习的实时目标检测算法，   
其核心思想是将目标检测问题转化为一个回归问题，   
通过单次前向传播即可完成目标定位和分类   

yolo的整体思想是，将输入图像划分为一个S*S的网格（例如分成4*4的网格）   
每个网格负责检测中心点落在该网格内的目标。每个网格会预测边界框和类别概率

边界框：
每个网格预测B个边界框，每个边界框包含5个值：
x,y,w,h,confidence
x和y是边界框的相对于网格的偏移量的中心坐标
w和h表示边界框的宽度和高度。这个高度是相对于整个图像的比例。
confidence是置信度，表示边界框内是否包含目标以及预测的准确性

类别概率
每个网格还会预测C个类别的概率
其中C是数据集中类别的总数
这里我们只有车牌识别一个类

在预测完成后，YOLO会输出一个S*S*（B*5+C）的张量

Yolo使用一个卷积神经网络CNN来提取图像特征并直接预测边界框和类别概率。
Yolo的损失函数由3部分组成：边界框坐标损失、置信度损失、类别损失

边界框坐标损失：计算预测边界框的中心坐标（x,y）和宽高（w,h）与真实值的误差
置信度损失：计算预测的置信度与真实值的误差。
类别损失：计算预测的类别概率与真实类别的误差。

损失函数的设计使得 YOLO 能够同时优化目标定位和分类任务。
对于我们这个项目，只有车牌一类，所以不需要分类。

YOLO会生成多个边界框，其中可能存在重叠的框。为了去除冗余的框，YOLO 使用非极大值抑制（NMS）
YOLO会根据置信度分数对边界框进行排序。
选择置信度最高的框，并移除与其重叠度(交并比)（IoU）超过一定阈值的其他框。
重复上述过程，直到所有框都被处理。

class 边界框
{
    float confidence;
};
vector<边界框> eraseBox(vector<边界框> boxes)
{
    eraseBox(vector<边界框> boxes);
}
void yolo_main()
{
    vector<边界框> boxes;
    boxes.sort_around_confidence();
    float iou_threshold=0.5;
    for(;;)
    {
        if(caluIOU(boxes[1],boxes[i]) > iou_threshold)
            boxes.erase(i);
    }
}
void yolo_main()
{
    vector<边界框> boxes;
    boxes.sort_around_confidence();
    float iou_threshold=0.5;
    vector<边界框> final=eraseBox(boxes);
}
这里给大家展示的只是伪代码，在使用yolo过程中我们并不需要详细理解具体是怎么实现的
如果大家对此感兴趣，可以在github上寻找yolo源代码进行学习




接下来介绍环境准备
首先我们需要一块Milk-V Duo开发板
同时我们需要PC主机的Linux环境，建议Ubuntu22.04系统
然后在Linux系统下配置Anaconda和Pycharm环境
然后配置PyTorch深度学习框架
上述的所有版本使用最新版即可。
具体的安装过程我们在下一期为大家展示
# 安装yolo
大家好，这期视频给大家展示YOLO的安装
打开ubuntu 首先安装anaconda


在哪里下载都可以
这里选择清华园镜像

下载好后运行sh脚本
首先给脚本加权限
然后才能运行
运行过程中，一路回车即可，有需要输入yes或no的直接输入yes。   
安装过程中只需要关注一下安装路径。默认的安装路径是/home/<Username>/anaconda3，这个路径是可以的。 
这里的<Username>指你的用户名。  

安装完成后我们需要来配置一下环境
sudo vim ~/.bashrc
在文件末尾添加如下内容
export PATH="/home/<Username>/anaconda3/bin:$PATH"
这里需要把<Username>替换成你的用户名



保存后退出
然后输入
source ~/.bashrc
更新下环境
然后输入conda list，检查是否下载完成
有包名，则表示下载完成。



配置完环境，我们需要创建虚拟环境
conda create -n <环境名称自定义> python=<python的版本号>
例如：
conda create -n My_torch python=3.10
输入后回车，然后
source activate My_torch


现在来安装pytorch
输入pytorch官网用来获取下载指令
打开网站后往下拉就会有 选择pip版本
选择cpu版本 然后得到安装命令





然后安装yolo
在git上搜索yolo，选择任意版本下载
这里为了方便测试，我使用yolo v5


输入指令进入环境：
conda activate <你的环境>
例如我的：
conda activate My_torch
使用指令用清华源安装需要的环境：
pip install -U -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
最后进行测试
通过VSCode打开yolo解压文件夹，
打开detect.py文件
设置python解释器，选择My_torch
点击运行，可以看到输出
测试运行的运行结果在该文件夹下的runs文件夹下，可进行查看
# 介绍Duo和RISC-V
大家好，这期视频介绍Duo和RISC-V，为我们后面的学习做准备   
   
首先介绍RISC-V   
现在比较热门的CPU架构有   
x86、ARM、MIPS、RISC-V等   
我们通常使用的电脑就是X86架构，x86架构在高性能计算领域具有一定的优势。   
手机等移动产品通常使用arm架构，arm架构具有低功耗的优势。   
MIPS通常用于特定领域，如网络设备。   
RISC-V则是开源指令集架构，具有模块化、可扩展性、简洁高效等特点。其指令集完全开源，设计简单，便于实现高效的硬件架构。   
RISCV因为开源的特性，使得社区比较活跃，在许多领域都有人研究，   
在嵌入式系统、高性能计算、物联网、人工智能、教育与研究等领域都有广泛的应用前景。   
这里面ARM、MIPS和RISC-V都是基于精简指令集（RISC）的计算机架构，通常在功耗上比复杂指令集（CISC）要低   
因为低功耗的特性，他们常用于嵌入式设备   
   
精简指令集，全称Reduced Instruction Set Computer，简称RISC，   
我们的RISC-V，代表了Berkeley大学（加州大学伯克利分校）设计的第五代RISC芯片   
RISC-V采用模块化设计，形如浮点运算、原子操作、压缩指令等汇编模块也可以选择性的添加   
模块化的特性使得risc five成为科研机构和企业关注的焦点。   
RISC-V的开放性使其成为教育和研究领域的理想选择。许多大学和研究机构采用RISC-V作为教学和研究平台，推动计算机体系结构的教育和创新。   

接下来介绍Duo
Milk-V Duo是一个基于CV1800B芯片的超紧凑嵌入式开发平台，处理器是1GHz和700MHz的RISC-V C906处理器。   
它可以运行Linux和RTOS，为专业人士、工业ODM、AIoT爱好者、DIY爱好者和创作者提供了一个可靠、低成本和高性能的平台。
它具有一个16针FPC接口，用于2-lane MIPI摄像头输入   
我们后续的课程就是基于此展开的   
   
# 录制素材和分类方法
大家好
该课题的内容是使用Milk-V Duo通过Yolo进行目标识别
在前面我们将上述所有都介绍完毕
现在就可以开始准备设计
既然是使用yolo，我们就需要提供训练集
所以我们需要录制合适的素材并将它们进行分类
在后面我们会用到FFmpeg来获取可以使用的数据集，
所以现在我们需要录制一段视频
可以用手机录制，也可以用相机
将我们录制的视频通过ffmpeg分割成一张张的图片
这就是我们的数据集。

现在我们确定一下要录制什么素材，以及分类方法
我们录制车道上的汽车，可以是驾驶中的，可以是停止的。最后我们需要在这些视频中截取他们的有车牌号的画面
要注意光线、背景和场景布置，以获得清晰、美观的画面，有利于yolo进行学习

我们在将视频截取出图像之后，需要对他们进行分类
我们可以按照图像的清晰度进行分类，将不是特别清晰的作为训练集，将比较清晰的作为测试集，用于yolo的学习
当然，可以按照任意其他的分类方式进行分类，不过通过清晰度进行分类并合理的设置更有利于学习

# X-AnyLabeling的安装与使用
大家好，这期视频带来的是标注软件X-AnyLabeling的安装与使用      
在安装之前，我们需要先检查一下python版本，做一下提前的准备   
这里我使用的是Ubuntu24.04，自带的python是3.12.3   
而X-AnyLabeling要求Python版本在3.10以上   

同时我们还需要去到X-AnyLabeling的官网，将包体下载下来   
官网链接：https://github.com/CVHub520/X-AnyLabeling/tree/main   
进入官网后我们有两种方式下载，一个是clone仓库，一个是下载zip压缩包   
我们使用clone的方式   





这里我已经下载好了   
接下来就开始做其他的准备   
首先我们需要创建Python的虚拟环境，需要使用到下列的命令   
sudo apt install python3.12-venv   
python3 -m venv x-anylabeling   
这里我们将在home目录下创建一个名为x-anglabeling的虚拟环境   
然后来激活虚拟环境   
source x-anylabeling/bin/activate   
接下来，我们需要在虚拟环境下安装一些包   
在虚拟环境下安装的好处是可以避免包体冲突，特别是对于系统引用的一些包体。   
pip install PyQt5   
pip install numpy   
pip install onnxruntime   

然后我们在终端中进入clone文件夹   
重新进入虚拟环境   
在桌面下   
source ../x-anylabeling/bin/activate   
然后我们需要安装X-AnyLabeling需要我们安装的   
这些都存在   
requirements文本文档内   
大家可以按照需要进行选择   
比如这里我选择的是CPU版本，我的命令就是   
pip install -r requirements.txt   
如果我需要使用GPU加速，那么我的命令就是   
pip install -r requirements-gpu.txt   

安装完成后，在clone文件夹下使用如下命令   
pyrcc5 -o anylabeling/resources/resources.py anylabeling/resources/resources.qrc   
同时，为了防止冲突，使用如下命令   
pip uninstall anylabeling -y   
这里显示我并没有安装任何anylabeling   

这样就准备好了所有条件   
接下来就是运行程序   
使用如下命令   
python anylabeling/app.py   
这样就完成了X-AnyLabeling的安装   
然后来介绍X-AnyLabeling的基本使用   







首先，软件的详细使用可以参考官方文档   
https://github.com/CVHub520/X-AnyLabeling/blob/main/docs/zh_cn/user_guide.md   
这里我们简单介绍一下   
首先是画面   
X-AnyLabeling完美支持中文      
   
在刚打开时，我们可以选择一个文件夹      
然后左边栏会被分成4部分         
第一部分负责文件操作      
第二部分负责手动标注      
第三部分是标签编辑页面   
第四部分是自动标注   
   
我们先尝试手动标注几个   
《第一个图可以简单标注一下》   
这样就标注好了   
   
这个软件的重点是AI自动标注   
我们来尝试一下   
点击这个AI按钮，就是自动标注按钮   
这个时候我们需要选择模型   
这里，模型可以自定义，也可以选择X-AnyLabeling提供的模型   
这里提供了非常多的模型，这些模型在第一次使用时需要加载   
这个速度可能很慢，甚至可能加载失败   
这时我们可以自定义模型，或者使用官方仓库提供的模型文件   
这里，官方也提供了参考的文档   
https://github.com/CVHub520/X-AnyLabeling/blob/v1.1.0/docs/custom_model.md   
我们可以根据我们需要的，来进行下载      
比如下载yolov8l的onnx和yaml文件   
官方甚至贴心的给我们提供了百度网盘的下载渠道   
这里我已经下载好了    
我们把下载好的文件放在一个合适的任意位置，然后选择加载自定义模型
如果这个时候，我们直接选择这个模型，仍然有很大的可能加载模型失败
这个时候我们需要打开yaml文件    
将里面的model_path设置成我们的onnx文件与yaml文件之间的相对路径   
然后再加载自定义模型   
这样就可以完成加载
这之后，我们就可以尝试使用AI功能。

然后我们尝试使用全部AI标注

# X-AnyLabeling的安装与使用下
大家好
前面我们通过FFmpeg截取到了许多张车辆运行过程中的画面。现在我们要用X-AngLabeling来将它们做上标记。
这个视频，我们更多的关注一下yaml文件的内容
来帮助我们在后面设置模型参数，进行模型训练教学，并导出 ONNX 模型。


首先这里我们得到了ffmpeg_source文件夹，里面存放了几百张通过行车记录仪录制视频的画面。我们通过X-AnyLabeling的AI自动标注对他进行一个标注，在这期间我们解释一下yaml文件的内容

打开yaml文件，可以看到有许多字段：
type
name
display_name
model_path
nms_threshold
confidence_threshold
classes

一一介绍
type：是模型类型标识，不可修改。   
name：是模型配置文件的索引名称，保留默认值即可，不可修改。   
display_name：是在界面上模型下拉列表中显示的名称，可自行修改。	   
model_path：是模型加载路径，支持相对路径和绝对路径。应当按照实际需求进行修改。要求是onnx格式的文件。
剩下的字段都是参数，具体的设置需要参考对应模型的定义。
nms_threshold：是非极大值抑制的阈值，用于过滤重叠的目标框。
confidence_threshold：置信度阈值，用于过滤置信度较低的目标框。
classes：模型的标签列表，需与训练时的标签列表一致。
这里还有两个字段没有展示：
filter_classes：指定推理时使用的类别。
agnostic：是否使用单类 NMS。


简单总结一下前面的视频的内容
yolo是一种目标检测模型，可以一次性识别图片中的物体的类别和位置。
在后面，我们将使用最新版的yolo，得到pt模型后，将pt模型转为onnx模型，再次对数据集进行标注。
# 设置模型参数
大家好，这期视频我们来设置模型参数
在进行之前，我们安装最新版的yolo

进入github
https://github.com/ultralytics/ultralytics    


打开控制台
克隆工程
```  bash
git clone https://github.com/ultralytics/ultralytics.git
```
接下来安装ultralytics
```   bash
pip install ultralytics
```
安装PyTorch
```   bash
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```


接下来使用命令行简单预测一下
```  bash
yolo predict model=yolo11n.pt imgsz=640 conf=0.25
```
这里会自动下载，需要点时间
我就直接将下载好的文件放在桌面了
运行完成后会在当前目录下生成两个文件：
runs目录和yolo11n.pt
进入终端中显示的目录就可以看到预测结果
runs/detected/predict

这里的重点是我们得到的yolo11n.pt
他在后面起着相当重要的作用



进入官网   
在Quickstart--Modifying Settings中
https://docs.ultralytics.com/quickstart/#modifying-settings
在这里有两个栏目比较重要
Modifying Settings和Understanding Settings
在UnderstandingSettings中我们可以看到Ultralytics环境可以设置的所有不同类型的设置
这里也提供了参数的合法值和设置的描述
这里我们选择比较直观的设置，来进行尝试
runs_dir：运行结果存储位置
vscode_msg：增加对VSCode的支持

```  python
from ultralytics import settings
settings.update({"runs_dir": "lizzy"})
settings.update({"vscode_msg": True})
```
在终端中运行python
执行相应的代码

然后再次运行命令
yolo predict model=yolo11n.pt imgsz=640 conf=0.25

# 导出onnx模型
大家好，这期视频我们导出onnx模型
在上一步我们已经获得了yolo11n.pt   
这一步我们将其转换为onnx文件   
onnx模型的好处是CPU速度能提高3倍   
打开官方文档https://docs.ultralytics.com/zh/modes/export/#usage-examples   
这里给出了使用示例和参数   
我们先看参数   
这里有这样几个参数需要注意   
format："onnx"
dynamic：True
imgsz:不设置
int8:True
simplify:True
device:cpu

然后，我们就可以根据示例修改参数，并生成onnx文件
```  python
from ultralytics import YOLO
model = YOLO("yolo11n.pt")  # load an official model
model.export(format="onnx",dynamic=True,int8=True,simplify=True)
```

这样就生成了onnx文件
接下来，我们在X-AnyLabeling中重新使用新生成的模型运行一次

在使用之前，我们需要给模型创建配置文件
因为官方还没有对最新版yolo的兼容，我们直接对着yolov8的配置文件修改即可

# 配置TPU_MLIR工作环境

安装Python3.10，必须是这个版本


安装docker
网站https://docs.docker.com/engine/install/ubuntu/   
卸载可能的旧版本docker   
```bash
for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done
sudo apt autoremove
```
开始安装Docker
```bash
sudo apt update
# 安装依赖包【用于通过HTTPS来获取仓库】
sudo apt install apt-transport-https ca-certificates curl software-properties-common

# 添加Docker官方GPG密钥
sudo -i #进入root
curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/trusted.gpg.d/docker-ce.gpg
logout # 退出root
# 验证。0EBFCD88 是公钥的指纹。执行这个命令后，系统会显示与该指纹相关的公钥信息。
sudo apt-key fingerprint 0EBFCD88


# 添加Docker阿里稳定版软件源
sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"
# 每次添加源时都需要更新软件源列表，也就是apt update
sudo apt update
# 安装默认最新版
sudo apt install docker-ce docker-ce-cli containerd.io
```
配置docker镜像源
```bash
sudo mkdir -p /etc/docker
sudo vim /etc/docker/daemon.json
```
编辑文件
```json
{
  "registry-mirrors":[
    "https://yxzrazem.mirror.aliyuncs.com",
    "https://registry.docker-cn.com",
    "https://docker.mirrors.ustc.edu.cn",
    "https://hub-mirror.c.163.com",
    "https://mirror.baidubce.com",
    "https://2a6bf1988cb6428c877f723ec7530dbc.mirror.swr.myhuaweicloud.com",
    "https://docker.m.daocloud.io",
    "https://your_preferred_mirror",
    "https://dockerhub.icu",
    "https://docker.registry.cyou",
    "https://docker-cf.registry.cyou",
    "https://dockercf.jsdelivr.fyi",
    "https://docker.jsdelivr.fyi",
    "https://dockertest.jsdelivr.fyi",
    "https://mirror.aliyuncs.com",
    "https://dockerproxy.com",
    "https://docker.m.daocloud.io",
    "https://docker.nju.edu.cn",
    "https://docker.mirrors.sjtug.sjtu.edu.cn",
    "https://docker.mirrors.ustc.edu.cn",
    "https://mirror.iscas.ac.cn",
    "https://docker.rainbond.cc",
    "https://y3qevhs5.mirror.aliyuncs.com"
  ]
}
```
编辑完成后更新
```bash
sudo systemctl daemon-reload
sudo systemctl restart docker
```
运行测试
```bash
# 测试，安装好后默认启动
sudo docker run hello-world
# 如果输出“Hello from Docker!”则表示Docker已经成功安装。
```
资源检查
```bash
# 查看有哪些镜像
sudo docker images
```
配置用户组
```bash
# 配置完成后不用再sudo了
sudo usermod -aG docker ${USER}  # 这里可以将${USER}替换成指定的用户名，如sudo usermod -aG docker lizzy,且建议后者
su - lizzy      # 刷新shell
docker images   # 验证
```
拉取镜像
```  bash
# 拉取镜像
docker pull sophgo/tpuc_dev:v3.2
# 新建容器
# 注意修改<myname>的值
# docker run --privileged --name <myname> -v $PWD:/workspace -it sophgo/tpuc_dev:v3.2
# 这里我是
docker run --privileged --name yolo -v $PWD:/workspace -it sophgo/tpuc_dev:v3.2
```
这一步之后，我们就会进入docker的workspace
```bash
root@d9e9aac2c4d7:/workspace# 
```



## 安装tpu_mlir
在docker下
```  bash
root@d9e9aac2c4d7:/workspace# pip install tpu_mlir
```
再安装一下tpu_mlir[onnx]
```bash
root@d9e9aac2c4d7:/workspace# pip install tpu_mlir[onnx]
```
下载模型转换工具包
```bash
root@d9e9aac2c4d7:/workspace# git clone https://github.com/milkv-duo/tpu-mlir.git
```
将包拉取到Workspace目录下后，运行脚本
```bash
root@d9e9aac2c4d7:/workspace# source ./tpu-mlir/envsetup.sh
```









## ONNX模型的编译和转换
首先下载文件
yolov5.onnx:https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.onnx   
tpu_mlir_resource:https://github.com/sophgo/tpu-mlir/releases/   
下载下来的文档名字是tpu-mlir-resource.tar，解压之后需要修改名称，中间的连接符修改为下划线：tpu_mlir_resource   
将解压后的文件夹和onnx模型放到~/tpu_mlir目录下
在~/tpu_mlir下打开终端，进行复制
```bash
docker cp ~/tpu_mlir/tpu_mlir_resource <myname>:/workspace/tpu_mlir_resource
docker cp ~/tpu_mlir/tpu_mlir_resource yolo:/workspace/tpu_mlir_resource
docker cp ~/tpu_mlir/yolov5s.onnx <myname>:/workspace/yolov5s.onnx
docker cp ~/tpu_mlir/yolov5s.onnx yolo:/workspace/yolov5s.onnx
docker cp ~/tpu_mlir/yolo11n.onnx yolo:/workspace/yolo11n.onnx  ###########
# 然后看一下docker中是不是存在
root@d9e9aac2c4d7:/workspace# ls   # 应该是有的
```
然后回到docker
在docker中，新建文件夹model_yolov5s用于存放相关文件以及进行转化操作
```bash
root@d9e9aac2c4d7:/workspace# mkdir model_yolov5s



root@d9e9aac2c4d7:/workspace# cp -rf yolov5s.onnx ./model_yolov5s
root@d9e9aac2c4d7:/workspace# cp -rf tpu_mlir_resource/regression/image ./model_yolov5s
root@d9e9aac2c4d7:/workspace# cp -rf tpu_mlir_resource/regression/dataset/COCO2017 ./model_yolov5s
root@d9e9aac2c4d7:/workspace# cd model_yolov5s
root@d9e9aac2c4d7:/workspace/model_yolov5s# ls
# 返回值3个： COCO2017   image   yolov5s.onnx

#再新建一个workspace文件夹
root@d9e9aac2c4d7:/workspace/model_yolov5s# mkdir workspace && cd workspace
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace#
```
### onnx模型转换成mlir模型
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace#
model_transform \
    --model_name yolov5s \
    --model_def ../yolov5s.onnx \
    --input_shapes [[1,3,640,640]] \
    --mean 0.0,0.0,0.0 \
    --scale 0.0039216,0.0039216,0.0039216 \
    --keep_aspect_ratio \
    --pixel_format rgb \
    # --output_names 350,498,646 \
    --test_input ../image/dog.jpg \
    --test_result yolov5s_top_outputs.npz \
    --mlir yolov5s.mlir
```
yolo11版本
```bash
model_transform \
    --model_name yolo11n \
    --model_def ../yolo11n.onnx \
    --input_shapes [[1,3,640,640]] \
    --mean 0.0,0.0,0.0 \
    --scale 0.0039216,0.0039216,0.0039216 \
    --keep_aspect_ratio \
    --pixel_format rgb \
    --test_input ../image/dog.jpg \
    --test_result yolo11n_top_outputs.npz \
    --mlir yolo11n.mlir

    ## --output_names 350,498,646 \  
```


然后会显示
[Success]:npz_tool.py ......
转换成功后会生成一个{model_name}_in_f32.npz文件。这里就是yolov5s_in_f32.npz文件



# 使用校准表生成 INT8 对称模型
量化int8
通过calibration得到转换int8时的校准表
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# 
run_calibration yolov5s.mlir \
    --dataset ../COCO2017 \
    --input_num 100 \
    -o yolov5s_cali_table
```

```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# 
run_calibration yolov8l.mlir \
    --dataset ../COCO2017 \
    --input_num 100 \
    -o yolov8l_cali_table
```

运行完成后会生成名为 yolov5s_cali_table 的文件，ls命令查看。   
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# ls
# 会显示yolov5s_cali_table
```

然后转成INT8对称量化模型   
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# 
model_deploy \
    --mlir yolov5s.mlir \
    --quantize INT8 \
    --calibration_table yolov5s_cali_table \
    --processor bm1684x \
    --test_input yolov5s_in_f32.npz \
    --test_reference yolov5s_top_outputs.npz \
    --tolerance 0.85,0.45 \
    --model yolov5s_1684x_int8_sym.bmodel
```
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# 
model_deploy \
    --mlir yolo11n.mlir \
    --quantize INT8 \
    --calibration_table yolo11n_cali_table \
    --processor cv180x \
    --test_input yolo11n_in_f32.npz \
    --test_reference yolo11n_top_outputs.npz \
    --tolerance 0.85,0.45 \
    --model yolo11n_cv180x_int8_sym.cvimodel
```
显示Success后即为转换成功

# 使用模型进行推理
在docker中
#### onnx
```bash
detect_yolov5 \
    --input ../image/dog.jpg \
    --model ../yolov5s.onnx \
    --output dog_onnx.jpg
```
#### int8 bmodel
```bash
detect_yolov5 \
    --input ../image/dog.jpg \
    --model yolov5s_cv180x_int8_sym.cvimodel \
    --output dog_int8_sym.jpg
```


































