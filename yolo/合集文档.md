# YOLO目标识别原理
## 介绍
YOLO是一种目标检测算法，以其高速度和实时性著称。
其核心思想是将目标检测视为回归问题，通过单次前向传播即可完成对图像中所有目标的定位和分类。
## 原理
### 一、核心原理与步骤
1. 图像网格划分
   YOLO将输入图像划分为S*S的网格（例如YOLOv1采用7×7网格）。每个网格单元负责检测中心点落在该区域内的目标，并预测多个边界框（Bounding Box）。

2. 边界框与类别预测
   每个网格单元预测B个边界框，每个框包含以下信息：（x,y,w,h,confidence）
   - 坐标：中心点相对网格的偏移量(x, y)和归一化的宽高(w, h)（相对于整张图像的比例）。
   - 置信度（Confidence）：表示框内存在目标的概率，并与预测框和真实框的交并比（IoU）相乘计算得出。
   除此之外，还有如下信息：
   - 类别概率：每个网格预测C个类别的概率，最终类别置信度为类别概率与边界框置信度的乘积
   - 返回值：在预测完成后，YOLO会输出一个S*S*（B*5+C）的张量

3. 置信度阈值与非极大值抑制（NMS）  
   - 阈值过滤：去除置信度低于设定阈值的边界框，减少冗余检测。
   - 非极大值抑制：对同一目标的多个重叠框，仅保留置信度最高的框。通过计算IoU剔除重复检测结果。

### 二、网络结构与训练评价

1. 网络架构  
   - 骨干网络：早期YOLO采用GoogLeNet或DarkNet作为特征提取器，后续版本如YOLOv3使用DarkNet-53，通过卷积层CNN逐步提取特征并生成预测张量。
   - 输出张量：最终输出为S*S*（B*5+C）。

2. 训练评价：损失函数  
   YOLO的损失函数由三部分组成：
   - 边界框坐标损失：计算预测边界框的中心坐标（x,y）和宽高（w,h）与真实值的误差
   - 置信度损失：计算预测的置信度与真实值的误差。
   - 类别损失：计算预测的类别概率与真实类别的误差。

## YOLO v1代码实现
https://e.coding.net/xucancan1/yolov1/YOLOv1.git


## 参考文档
https://blog.csdn.net/hgnuxc_1993/article/details/116945869
https://www.ctyun.cn/developer/article/435054236770373
https://www.cnblogs.com/fariver/p/7446921.html
https://bbs.huaweicloud.com/blogs/297764
https://www.cnblogs.com/fariver/p/7446921.html


# 安装yolo
## anaconda 
### 第一步：安装ubuntu主机系统或虚拟机系统   
### 第二部：安装Anaconda3   
在清华源网址下载Anaconda3-2021.11-Linux-x86_64.sh   
【清华源】https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/    
Ctrl+F Anaconda3-2021.11-Linux-x86_64.sh   
点击直接下载   
下载好后运行sh文件，首先chmod加权限，然后直接运行   
运行过程中，一路回车即可，有需要输入yes或no的直接输入yes。   
安装过程中只需要关注一下安装路径。默认的安装路径是/home/<Username>/anaconda3，这个路径是可以的。 
这里的<Username>指你的用户名。  
一路安装即可。   
### 第三部：环境配置
```sudo vim ~/.bashrc```
在文件末尾添加如下内容
```export PATH="/home/<Username>/anaconda3/bin:$PATH"```
这里需要把<Username>替换成你的用户名
我就是
```export PATH="/home/lizzy/anaconda3/bin:$PATH"```
保存后退出
然后输入
```source ~/.bashrc```
更新下环境
然后输入```conda list```，检查是否下载完成
有包名，则表示下载完成。
### 第四步：切换镜像
由于annaconda自带的下载工具pip默认使用的是外网的网址，接下来需要对其网址进行更新，用我们国的自带的网址，这样使用conda pip就非常快
```pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple```
由于annaconda也自带的conda工具默认使用的是外网的网址，我们也需要对其进行配置，方便接下来的环境管理与使用
```
conda clean -i
sudo vim ~/.condarc
```
进入配置文件后，直接用下面的配置信息进行替换
```
channels:
 - defaults
show_channel_urls: true
default_channels:
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
 conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
 msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
 bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
 menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
 pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
 pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
 simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
```
输入conda安装第三方包测试：
```conda install scrapy```
测试时间比较长，大概3min
### 第五部：创建虚拟环境
```
conda create -n <环境名称自定义> python=<python的版本号>
```
例如：
```
conda create -n My_torch python=3.10
```
输入后回车，然后
```
source activate My_torch
```
### 第六步：finish
启用虚拟环境
```
conda activate <自定义的环境名称>
```
例如：
```
conda activate My_torch
```


## yolo
### 1.安装PyTorch
在linux中打开火狐浏览器里输入pytorch官网用来获取下载指令
【Pytorch官网】https://pytorch.org/
打开网站后往下拉就会有，
选择pip版本，选择cpu版本，下面得到安装命令
```
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```
### 2.下载yolo
https://gitee.com/monkeycc/yolov5   
下载zip
解压到桌面下，并从终端进入文件夹
source ~/.bashrc
输入指令进入环境：
```
conda activate <你的环境>
```
例如我的：
```
conda activate My_torch
```
使用指令用清华源安装需要的环境：
```
pip install -U -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```
### 3.测试
通过VSCode打开yolo解压文件夹，
打开detect.py文件
设置python解释器，选择My_torch
点击运行，可以看到输出

## yoloV11
https://blog.csdn.net/StopAndGoyyy/article/details/143169639
https://github.com/ultralytics/ultralytics


# X-AnyLabeling的安装与使用
## 安装与运行
### 配置环境
python3.10以上，下文使用的是ubuntu24.04自带的3.12.3     

```
sudo apt install python3.12-venv
```
创建虚拟环境，将虚拟环境创建在~下
```
~$ python3 -m venv x-anylabeling
```
激活虚拟环境
```
source x-anylabeling/bin/activate  #linux
x-anylabeling\Scripts\activate     #win
```
### 在虚拟环境下安装onnx
pip install PyQt5
pip install numpy
pip install onnxruntime
### 下载AnyLabeling
官网链接：https://github.com/CVHub520/X-AnyLabeling/tree/main   
安装链接：https://github.com/CVHub520/X-AnyLabeling/releases   
两个版本：Linux与Windows    
无论是哪个，都建议使用git clone     
```
## 扒库
git clone git@github.com:CVHub520/X-AnyLabeling.git   
## 进入cmd或terminal，进入clone文件夹
pip install -r requirements.txt
pip install -r requirements-gpu.txt
## 在clone文件夹下
pyrcc5 -o anylabeling/resources/resources.py anylabeling/resources/resources.qrc
## 防止冲突
pip uninstall anylabeling -y
```

## 运行
### 安装完成后运行app.py
python anylabeling/app.py












## 使用
软件的详细使用可以参考官方文档   
https://github.com/CVHub520/X-AnyLabeling/blob/main/docs/zh_cn/user_guide.md   
模型文件   
https://github.com/CVHub520/X-AnyLabeling/blob/v1.1.0/docs/custom_model.md   
模型文件需要下载的有yaml文件和onnx文件    
需要打开yaml文件，给modle_path设置为onnx的路径

每标注了一张图片，就会在指定文件夹（默认为当前文件夹）下生成对应的json文件   


# 设置模型参数
## 安装yolo最新版
### Ultralystic官网与github   
https://docs.ultralytics.com/zh#what-is-ultralytics-yolo-and-how-does-it-improve-object-detection   
官方github：https://github.com/ultralytics/ultralytics    
### 安装PyTorch
https://pytorch.org/get-started/locally/   
```   bash
# Linux Pip Python CPU
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```
通过PyTorch检查是否是GPU版本
```   python
# 在python中
import torch
torch.cuda.is_avaliable()  #返回一个True或False
```
### 回到Ultralystic官网
接下来我们关注
Use Ultralystic with CLI
训练预测验证、导出、特殊模式

简单预测一下
直接在终端中运行（最好在虚拟环境下）
```  bash
yolo predict model=yolo11n.pt imgsz=640 conf=0.25
```
运行后得到一个目录和模型文件
目录是生成位置，在终端中有所显示，默认的应该是/runs/detected/predict
模型文件的文件名是yolo11n.pt

## 参数设置
设置模型参数   
具体先看有哪些参数可以进行设置   
比如将结果存储在其他目录中   


进入官网   
在Quickstart--Modifying Settings中
https://docs.ultralytics.com/quickstart/#modifying-settings
在这里有两个栏目比较重要
ModifyingSettings和UnderstandingSettings
在UnderstandingSettings中我们可以看到Ultralytics环境可以设置的所有不同类型的设置
这里也提供了参数的合法值和设置的描述
我们可以尝试设置几个
datasets_dir：数据集存储位置
weights_dir：权重文件存储位置
runs_dir：运行结果存储位置
vscode_msg：增加对VSCode的支持
```  python
from ultralytics import settings

## Update a setting
# settings.update({"runs_dir": "/path/to/runs"})

## Update multiple settings
# settings.update({"runs_dir": "/path/to/runs", "tensorboard": False})

## Reset settings to default values
# settings.reset()
from ultralytics import settings
settings.update({"runs_dir": "lizzy/runs"}) #生成路径为/lizzy/runs/detect/predict
settings.update({"weights_dir": "lizzy/weight"})
settings.update({"datasets_dir": "lizzy/dataset"})
settings.update({"vscode_msg": True})
```


# 导出onnx模型
官方文档：https://docs.ultralytics.com/zh/modes/export/#usage-examples   
在上一步我们已经获得了yolo11n.pt
这一步我们将其转换为onnx文件

在转换时，使用python进行转换
示例代码
```  python
from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load an official model
model = YOLO("path/to/best.pt")  # load a custom trained model

# Export the model
model.export(format="onnx")
```
这里，export函数有许多参数可以使用
官方文档中也提供了所有参数
我们需要注意的几个参数有：
format："onnx"
dynamic：True
imgsz:不设置
int8:True
simplify:True
device:cpu

运行export后，就会生成onnx文件


在X-AnyLabeling中使用时，我们需要修改yaml文件
因为官方还没有对最新版yolo的兼容，我们直接对着yolov8的配置文件修改即可
yaml文件
```
type: yolov8
name: yolov8l-r20230520
display_name: YOLO11n
model_path: yolo11n.onnx
nms_threshold: 0.45
confidence_threshold: 0.25
classes:
  - person
  - bicycle
```


# Ubuntu安装docker
安装docker
网站https://docs.docker.com/engine/install/ubuntu/   
卸载可能的旧版本docker   
```bash
for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done
sudo apt autoremove
```
开始安装Docker
```bash
sudo apt update
# 安装依赖包【用于通过HTTPS来获取仓库】
sudo apt install apt-transport-https ca-certificates curl software-properties-common

# 添加Docker官方GPG密钥
sudo -i #进入root
curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/trusted.gpg.d/docker-ce.gpg
logout # 退出root
# 验证。0EBFCD88 是公钥的指纹。执行这个命令后，系统会显示与该指纹相关的公钥信息。
sudo apt-key fingerprint 0EBFCD88


# 添加Docker阿里稳定版软件源
sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"
# 每次添加源时都需要更新软件源列表，也就是apt update
sudo apt update
# 安装默认最新版
sudo apt install docker-ce docker-ce-cli containerd.io
```
配置docker镜像源
```bash
sudo mkdir -p /etc/docker
sudo vim /etc/docker/daemon.json
```
编辑文件
```json
{
  "registry-mirrors":[
    "https://yxzrazem.mirror.aliyuncs.com",
    "https://registry.docker-cn.com",
    "https://docker.mirrors.ustc.edu.cn",
    "https://hub-mirror.c.163.com",
    "https://mirror.baidubce.com",
    "https://2a6bf1988cb6428c877f723ec7530dbc.mirror.swr.myhuaweicloud.com",
    "https://docker.m.daocloud.io",
    "https://your_preferred_mirror",
    "https://dockerhub.icu",
    "https://docker.registry.cyou",
    "https://docker-cf.registry.cyou",
    "https://dockercf.jsdelivr.fyi",
    "https://docker.jsdelivr.fyi",
    "https://dockertest.jsdelivr.fyi",
    "https://mirror.aliyuncs.com",
    "https://dockerproxy.com",
    "https://docker.m.daocloud.io",
    "https://docker.nju.edu.cn",
    "https://docker.mirrors.sjtug.sjtu.edu.cn",
    "https://docker.mirrors.ustc.edu.cn",
    "https://mirror.iscas.ac.cn",
    "https://docker.rainbond.cc",
    "https://y3qevhs5.mirror.aliyuncs.com"
  ]
}
```
编辑完成后更新
```bash
sudo systemctl daemon-reload
sudo systemctl restart docker
```
运行测试
```bash
# 测试，安装好后默认启动
sudo docker run hello-world
# 如果输出“Hello from Docker!”则表示Docker已经成功安装。
```
资源检查
```bash
# 查看有哪些镜像
sudo docker images
```
配置用户组
```bash
# 配置完成后不用再sudo了
sudo usermod -aG docker ${USER}  # 这里可以将${USER}替换成指定的用户名，如sudo usermod -aG docker lizzy,且建议后者
su - lizzy      # 刷新shell
docker images   # 验证
```
其他docker命令
```bash
# docker系统命令
sudo systemctl status docker    # 查看状态
sudo systemctl start docker     # 启动
sudo systemctl enable docker    # 开机自启
sudo systemctl stop docker      # 停止
# docker创建的容器
docker ps -a
# 容器的启动停止与控制
# 下面的<>的内容都是docker ps返回值中的列名
docker start <NAMES>   # 启动容器
docker stop <NAMES>    # 停止容器
docker exec -it <NAMES> <COMMAND> # 进入容器终端，需要先启动容器
```
# 配置TPU_MLIR
## 安装tpu_mlir
在docker下
```  bash
root@d9e9aac2c4d7:/workspace# pip install tpu_mlir
```
再安装一下tpu_mlir[onnx]
```bash
root@d9e9aac2c4d7:/workspace# pip install tpu_mlir[onnx]
```
下载模型转换工具包
```bash
root@d9e9aac2c4d7:/workspace# git clone https://github.com/milkv-duo/tpu-mlir.git
```
将包拉取到Workspace目录下后，运行脚本
```bash
root@d9e9aac2c4d7:/workspace# source ./tpu-mlir/envsetup.sh
```









## ONNX模型的编译和转换
首先下载文件
yolov5.onnx:https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.onnx   
tpu_mlir_resource:https://github.com/sophgo/tpu-mlir/releases/   
下载下来的文档名字是tpu-mlir-resource.tar，解压之后需要修改名称，中间的连接符修改为下划线：tpu_mlir_resource   
将解压后的文件夹和onnx模型放到~/tpu_mlir目录下
在~/tpu_mlir下打开终端，进行复制
```bash
docker cp ~/tpu_mlir/tpu_mlir_resource <myname>:/workspace/tpu_mlir_resource
docker cp ~/tpu_mlir/tpu_mlir_resource yolo:/workspace/tpu_mlir_resource
docker cp ~/tpu_mlir/yolov5s.onnx <myname>:/workspace/yolov5s.onnx
docker cp ~/tpu_mlir/yolov5s.onnx yolo:/workspace/yolov5s.onnx
docker cp ~/tpu_mlir/yolo11n.onnx yolo:/workspace/yolo11n.onnx  ###########
# 然后看一下docker中是不是存在
root@d9e9aac2c4d7:/workspace# ls   # 应该是有的
```
然后回到docker
在docker中，新建文件夹model_yolov5s用于存放相关文件以及进行转化操作
```bash
root@d9e9aac2c4d7:/workspace# mkdir model_yolov5s



root@d9e9aac2c4d7:/workspace# cp -rf yolov5s.onnx ./model_yolov5s
root@d9e9aac2c4d7:/workspace# cp -rf tpu_mlir_resource/regression/image ./model_yolov5s
root@d9e9aac2c4d7:/workspace# cp -rf tpu_mlir_resource/regression/dataset/COCO2017 ./model_yolov5s
root@d9e9aac2c4d7:/workspace# cd model_yolov5s
root@d9e9aac2c4d7:/workspace/model_yolov5s# ls
# 返回值3个： COCO2017   image   yolov5s.onnx

#再新建一个workspace文件夹
root@d9e9aac2c4d7:/workspace/model_yolov5s# mkdir workspace && cd workspace
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace#
```
### onnx模型转换成mlir模型
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace#
model_transform \
    --model_name yolov5s \
    --model_def ../yolov5s.onnx \
    --input_shapes [[1,3,640,640]] \
    --mean 0.0,0.0,0.0 \
    --scale 0.0039216,0.0039216,0.0039216 \
    --keep_aspect_ratio \
    --pixel_format rgb \
    # --output_names 350,498,646 \
    --test_input ../image/dog.jpg \
    --test_result yolov5s_top_outputs.npz \
    --mlir yolov5s.mlir
```
yolo11版本
```bash
model_transform \
    --model_name yolo11n \
    --model_def ../yolo11n.onnx \
    --input_shapes [[1,3,640,640]] \
    --mean 0.0,0.0,0.0 \
    --scale 0.0039216,0.0039216,0.0039216 \
    --keep_aspect_ratio \
    --pixel_format rgb \
    --test_input ../image/dog.jpg \
    --test_result yolo11n_top_outputs.npz \
    --mlir yolo11n.mlir

    ## --output_names 350,498,646 \  
```


然后会显示
[Success]:npz_tool.py ......
转换成功后会生成一个{model_name}_in_f32.npz文件。这里就是yolov5s_in_f32.npz文件



# 使用校准表生成 INT8 对称模型
量化int8
通过calibration得到转换int8时的校准表
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# 
run_calibration yolov5s.mlir \
    --dataset ../COCO2017 \
    --input_num 100 \
    -o yolov5s_cali_table
```

```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# 
run_calibration yolov8l.mlir \
    --dataset ../COCO2017 \
    --input_num 100 \
    -o yolov8l_cali_table
```

运行完成后会生成名为 yolov5s_cali_table 的文件，ls命令查看。   
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# ls
# 会显示yolov5s_cali_table
```

然后转成INT8对称量化模型   
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# 
model_deploy \
    --mlir yolov5s.mlir \
    --quantize INT8 \
    --calibration_table yolov5s_cali_table \
    --processor bm1684x \
    --test_input yolov5s_in_f32.npz \
    --test_reference yolov5s_top_outputs.npz \
    --tolerance 0.85,0.45 \
    --model yolov5s_1684x_int8_sym.bmodel
```
```bash
root@d9e9aac2c4d7:/workspace/model_yolov5s/workspace# 
model_deploy \
    --mlir yolo11n.mlir \
    --quantize INT8 \
    --calibration_table yolo11n_cali_table \
    --processor cv180x \
    --test_input yolo11n_in_f32.npz \
    --test_reference yolo11n_top_outputs.npz \
    --tolerance 0.85,0.45 \
    --model yolo11n_cv180x_int8_sym.cvimodel
```
显示Success后即为转换成功

# 使用模型进行推理
在docker中
#### onnx
```bash
detect_yolov5 \
    --input ../image/dog.jpg \
    --model ../yolov5s.onnx \
    --output dog_onnx.jpg
```
#### int8 bmodel
```bash
detect_yolov5 \
    --input ../image/dog.jpg \
    --model yolov5s_cv180x_int8_sym.cvimodel \
    --output dog_int8_sym.jpg
```















































